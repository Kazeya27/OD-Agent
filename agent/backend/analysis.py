#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Analysis functions for flow and corridor analysis
"""

from typing import Dict, Optional
import pandas as pd
from functools import lru_cache
from database import get_db, T_PLACES, T_DYNA


@lru_cache(maxsize=128)
def _get_city_province_mapping() -> Dict[int, str]:
    """è·å–åŸå¸‚åˆ°çœä»½çš„æ˜ å°„å…³ç³»ï¼Œä½¿ç”¨ç¼“å­˜é¿å…é‡å¤æŸ¥è¯¢"""
    with get_db() as conn:
        query = f"SELECT geo_id, province FROM {T_PLACES} WHERE province IS NOT NULL AND province != ''"
        rows = conn.execute(query).fetchall()
        return {row["geo_id"]: row["province"] for row in rows}


def _precompute_city_flow(
    start: str,
    end: str,
    direction: str = "send",
    dyna_type: Optional[str] = None,
) -> pd.DataFrame:
    """
    é¢„è®¡ç®—åŸå¸‚çº§åˆ«çš„æµé‡æ•°æ®

    Returns:
        DataFrame with columns: ['time', 'city_id', 'flow']
    """
    with get_db() as conn:
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        where_conditions = ["d.time >= ?", "d.time < ?"]
        params = [start, end]

        if dyna_type:
            where_conditions.append("d.type = ?")
            params.append(dyna_type)

        where_clause = " AND ".join(where_conditions)

        # é€‰æ‹©èšåˆå­—æ®µ
        group_field = "d.origin_id" if direction == "send" else "d.destination_id"

        query = f"""
            SELECT d.time, {group_field} as city_id, SUM(d.flow) as flow
            FROM {T_DYNA} d
            WHERE {where_clause}
            GROUP BY d.time, {group_field}
            ORDER BY d.time ASC, flow DESC
        """

        rows = conn.execute(query, params).fetchall()

        if not rows:
            return pd.DataFrame(columns=["time", "city_id", "flow"])

        data = []
        for r in rows:
            data.append(
                {
                    "time": str(r["time"]),
                    "city_id": int(r["city_id"]),
                    "flow": float(r["flow"]) if r["flow"] is not None else 0.0,
                }
            )

        return pd.DataFrame(data)


def analyze_province_flow_optimized(
    period_type: str,
    start: str,
    end: str,
    date_mode: str = "daily",
    direction: str = "send",
    dyna_type: Optional[str] = None,
) -> pd.DataFrame:
    """
    ä¼˜åŒ–ç‰ˆæœ¬çš„çœçº§æµé‡åˆ†æ - é€šè¿‡é¢„è®¡ç®—åŸå¸‚æµé‡æ¥åŠ é€Ÿ

    Args:
        period_type: Period type identifier
        start: Start time (ISO8601)
        end: End time (ISO8601)
        date_mode: 'daily' | 'total'
        direction: 'send' | 'receive'
        dyna_type: Optional dyna.type filter

    Returns:
        DataFrame(columns=['province', 'date', 'flow', 'rank'])
    """
    # 1. é¢„è®¡ç®—åŸå¸‚æµé‡
    city_flow_df = _precompute_city_flow(start, end, direction, dyna_type)

    if city_flow_df.empty:
        return pd.DataFrame(columns=["province", "date", "flow", "rank"])

    # 2. è·å–åŸå¸‚åˆ°çœä»½çš„æ˜ å°„
    city_province_map = _get_city_province_mapping()

    # 3. å°†åŸå¸‚æµé‡æ˜ å°„åˆ°çœä»½
    city_flow_df["province"] = city_flow_df["city_id"].map(
        lambda x: city_province_map.get(x, "Unknown")
    )

    # 4. æŒ‰çœä»½èšåˆ
    if date_mode == "daily":
        result = city_flow_df.groupby(["time", "province"])["flow"].sum().reset_index()
        result.columns = ["date", "province", "flow"]
        result["rank"] = (
            result.groupby("date")["flow"]
            .rank(ascending=False, method="min")
            .astype(int)
        )
    else:  # total
        result = city_flow_df.groupby("province")["flow"].sum().reset_index()
        result.columns = ["province", "flow"]
        result["date"] = None
        result["rank"] = result["flow"].rank(ascending=False, method="min").astype(int)

    result = result.sort_values("rank")
    return result


def create_performance_indexes():
    """åˆ›å»ºæ€§èƒ½ä¼˜åŒ–æ‰€éœ€çš„æ•°æ®åº“ç´¢å¼•"""
    with get_db() as conn:
        # ä¸º dyna è¡¨åˆ›å»ºå¤åˆç´¢å¼•
        conn.execute(
            f"""
            CREATE INDEX IF NOT EXISTS idx_dyna_time_type_origin 
            ON {T_DYNA} (time, type, origin_id)
        """
        )

        conn.execute(
            f"""
            CREATE INDEX IF NOT EXISTS idx_dyna_time_type_destination 
            ON {T_DYNA} (time, type, destination_id)
        """
        )

        # ä¸º places è¡¨åˆ›å»ºç´¢å¼•
        conn.execute(
            f"""
            CREATE INDEX IF NOT EXISTS idx_places_geo_id_province 
            ON {T_PLACES} (geo_id, province)
        """
        )

        conn.commit()
        print("âœ… æ€§èƒ½ç´¢å¼•åˆ›å»ºå®Œæˆ")


def analyze_province_flow(
    period_type: str,
    start: str,
    end: str,
    date_mode: str = "daily",
    direction: str = "send",
    dyna_type: Optional[str] = None,
) -> pd.DataFrame:
    """
    Calculate province-level flow intensity and ranking
    ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬ä»¥æé«˜æ€§èƒ½

    Args:
        period_type: Period type identifier
        start: Start time (ISO8601)
        end: End time (ISO8601)
        date_mode: 'daily' | 'total'
        direction: 'send' | 'receive'
        dyna_type: Optional dyna.type filter

    Returns:
        DataFrame(columns=['province', 'date', 'flow', 'rank'])
    """
    # ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬
    return analyze_province_flow_optimized(
        period_type, start, end, date_mode, direction, dyna_type
    )


def analyze_province_flow_original(
    period_type: str,
    start: str,
    end: str,
    date_mode: str = "daily",
    direction: str = "send",
    dyna_type: Optional[str] = None,
) -> pd.DataFrame:
    """
    åŸå§‹ç‰ˆæœ¬çš„çœçº§æµé‡åˆ†æï¼ˆä¿ç•™ç”¨äºå¯¹æ¯”ï¼‰
    """
    with get_db() as conn:
        # Query data with province information
        if dyna_type:
            query = f"""
                SELECT d.time, d.origin_id, d.destination_id, d.flow, 
                       p1.province as origin_province, p2.province as destination_province
                FROM {T_DYNA} d
                LEFT JOIN {T_PLACES} p1 ON d.origin_id = p1.geo_id
                LEFT JOIN {T_PLACES} p2 ON d.destination_id = p2.geo_id
                WHERE d.time >= ? AND d.time < ? AND d.type = ?
                ORDER BY d.time ASC;
            """
            rows = conn.execute(query, (start, end, dyna_type)).fetchall()
        else:
            query = f"""
                SELECT d.time, d.origin_id, d.destination_id, d.flow,
                       p1.province as origin_province, p2.province as destination_province
                FROM {T_DYNA} d
                LEFT JOIN {T_PLACES} p1 ON d.origin_id = p1.geo_id
                LEFT JOIN {T_PLACES} p2 ON d.destination_id = p2.geo_id
                WHERE d.time >= ? AND d.time < ?
                ORDER BY d.time ASC;
            """
            rows = conn.execute(query, (start, end)).fetchall()

        # Convert to list
        data = []
        for r in rows:
            data.append(
                {
                    "time": str(r["time"]),
                    "origin_province": (
                        str(r["origin_province"]) if r["origin_province"] else "Unknown"
                    ),
                    "destination_province": (
                        str(r["destination_province"])
                        if r["destination_province"]
                        else "Unknown"
                    ),
                    "flow": float(r["flow"]) if r["flow"] is not None else 0.0,
                }
            )

        if not data:
            return pd.DataFrame(columns=["province", "date", "flow", "rank"])

        df = pd.DataFrame(data)

        # Choose aggregation dimension based on direction
        group_col = "origin_province" if direction == "send" else "destination_province"

        # Aggregate by date_mode
        if date_mode == "daily":
            result = df.groupby(["time", group_col])["flow"].sum().reset_index()
            result.columns = ["date", "province", "flow"]
            result["rank"] = (
                result.groupby("date")["flow"]
                .rank(ascending=False, method="min")
                .astype(int)
            )
        else:  # total
            result = df.groupby(group_col)["flow"].sum().reset_index()
            result.columns = ["province", "flow"]
            result["date"] = None
            result["rank"] = (
                result["flow"].rank(ascending=False, method="min").astype(int)
            )

        result = result.sort_values("rank")
        return result


def analyze_city_flow(
    period_type: str,
    start: str,
    end: str,
    date_mode: str = "daily",
    direction: str = "send",
    dyna_type: Optional[str] = None,
) -> pd.DataFrame:
    """
    Calculate city-level flow intensity and ranking

    Returns:
        DataFrame(columns=['city', 'date', 'flow', 'rank'])
    """
    with get_db() as conn:
        # æ„å»ºè¿‡æ»¤æ¡ä»¶
        where_parts = ["d.time >= ?", "d.time < ?"]
        params = [start, end]
        if dyna_type:
            where_parts.append("d.type = ?")
            params.append(dyna_type)
        where_clause = " AND ".join(where_parts)

        # æ–¹å‘ï¼šå‘é€/æ¥æ”¶
        city_join = (
            "LEFT JOIN {places} p ON d.origin_id = p.geo_id"
            if direction == "send"
            else "LEFT JOIN {places} p ON d.destination_id = p.geo_id"
        ).format(places=T_PLACES)

        if date_mode == "daily":
            # åœ¨ SQL ç«¯å®Œæˆèšåˆï¼Œæ˜¾è‘—å‡å°‘æ•°æ®é‡
            query = f"""
                SELECT d.time AS date,
                       COALESCE(p.name, 'Unknown') AS city,
                       SUM(d.flow) AS flow
                FROM {T_DYNA} d
                {city_join}
                WHERE {where_clause}
                GROUP BY d.time, city
                ORDER BY d.time ASC
            """
            rows = conn.execute(query, params).fetchall()
            if not rows:
                return pd.DataFrame(columns=["city", "date", "flow", "rank"])

            result = pd.DataFrame(
                [
                    {
                        "date": str(r["date"]),
                        "city": str(r["city"]),
                        "flow": float(r["flow"]) if r["flow"] is not None else 0.0,
                    }
                    for r in rows
                ]
            )
            result["rank"] = (
                result.groupby("date")["flow"]
                .rank(ascending=False, method="min")
                .astype(int)
            )
        else:
            # total æ¨¡å¼ï¼šä»…æŒ‰åŸå¸‚èšåˆ
            query = f"""
                SELECT COALESCE(p.name, 'Unknown') AS city,
                       SUM(d.flow) AS flow
                FROM {T_DYNA} d
                {city_join}
                WHERE {where_clause}
                GROUP BY city
            """
            rows = conn.execute(query, params).fetchall()
            if not rows:
                return pd.DataFrame(columns=["city", "date", "flow", "rank"])
            result = pd.DataFrame(
                [
                    {
                        "city": str(r["city"]),
                        "flow": float(r["flow"]) if r["flow"] is not None else 0.0,
                    }
                    for r in rows
                ]
            )
            result["date"] = None
            result["rank"] = (
                result["flow"].rank(ascending=False, method="min").astype(int)
            )

        result = result.sort_values("rank")
        return result


def analyze_province_corridor(
    period_type: str,
    start: str,
    end: str,
    date_mode: str = "total",
    topk: int = 10,
    dyna_type: Optional[str] = None,
) -> pd.DataFrame:
    """
    Calculate inter-province corridor flow intensity and ranking

    Returns:
        DataFrame(columns=['send_province', 'arrive_province', 'flow', 'rank'])
    """
    with get_db() as conn:
        # æ„å»ºè¿‡æ»¤æ¡ä»¶
        where_parts = ["d.time >= ?", "d.time < ?"]
        params = [start, end]
        if dyna_type:
            where_parts.append("d.type = ?")
            params.append(dyna_type)
        where_clause = " AND ".join(where_parts)

        # åœ¨ SQL ç«¯å®Œæˆèšåˆï¼Œæ˜¾è‘—å‡å°‘æ•°æ®é‡
        query = f"""
            SELECT COALESCE(p1.province, 'Unknown') AS send_province,
                   COALESCE(p2.province, 'Unknown') AS arrive_province,
                   SUM(d.flow) AS flow
            FROM {T_DYNA} d
            LEFT JOIN {T_PLACES} p1 ON d.origin_id = p1.geo_id
            LEFT JOIN {T_PLACES} p2 ON d.destination_id = p2.geo_id
            WHERE {where_clause}
            GROUP BY send_province, arrive_province
            ORDER BY flow DESC
            LIMIT ?
        """

        params.append(topk)
        rows = conn.execute(query, params).fetchall()

        if not rows:
            return pd.DataFrame(
                columns=["send_province", "arrive_province", "flow", "rank"]
            )

        # æ„å»ºç»“æœ DataFrame
        result = pd.DataFrame(
            [
                {
                    "send_province": str(r["send_province"]),
                    "arrive_province": str(r["arrive_province"]),
                    "flow": float(r["flow"]) if r["flow"] is not None else 0.0,
                }
                for r in rows
            ]
        )

        # æ·»åŠ æ’åï¼ˆå› ä¸ºå·²ç»æŒ‰æµé‡é™åºæ’åˆ—ï¼‰
        result["rank"] = range(1, len(result) + 1)

        return result


def analyze_city_corridor(
    period_type: str,
    start: str,
    end: str,
    date_mode: str = "total",
    topk_intra: int = 10,
    topk_inter: int = 30,
    dyna_type: Optional[str] = None,
) -> Dict[str, pd.DataFrame]:
    """
    Calculate city-level corridor flow (intra-province and inter-province)

    Returns:
        {'intra_province': df1, 'inter_province': df2}
    """
    with get_db() as conn:
        # æ„å»ºè¿‡æ»¤æ¡ä»¶
        where_parts = ["d.time >= ?", "d.time < ?"]
        params = [start, end]
        if dyna_type:
            where_parts.append("d.type = ?")
            params.append(dyna_type)
        where_clause = " AND ".join(where_parts)

        # æŸ¥è¯¢çœå†…èµ°å»Šï¼ˆåœ¨ SQL ç«¯å®Œæˆèšåˆå’Œè¿‡æ»¤ï¼‰
        intra_query = f"""
            SELECT COALESCE(p1.name, 'Unknown') AS send_city,
                   COALESCE(p2.name, 'Unknown') AS arrive_city,
                   SUM(d.flow) AS flow
            FROM {T_DYNA} d
            LEFT JOIN {T_PLACES} p1 ON d.origin_id = p1.geo_id
            LEFT JOIN {T_PLACES} p2 ON d.destination_id = p2.geo_id
            WHERE {where_clause}
              AND COALESCE(p1.province, '') = COALESCE(p2.province, '')
              AND COALESCE(p1.province, '') != ''
            GROUP BY send_city, arrive_city
            ORDER BY flow DESC
            LIMIT ?
        """

        intra_rows = conn.execute(intra_query, params + [topk_intra]).fetchall()

        # æŸ¥è¯¢çœé™…èµ°å»Šï¼ˆåœ¨ SQL ç«¯å®Œæˆèšåˆå’Œè¿‡æ»¤ï¼‰
        inter_query = f"""
            SELECT COALESCE(p1.name, 'Unknown') AS send_city,
                   COALESCE(p2.name, 'Unknown') AS arrive_city,
                   SUM(d.flow) AS flow
            FROM {T_DYNA} d
            LEFT JOIN {T_PLACES} p1 ON d.origin_id = p1.geo_id
            LEFT JOIN {T_PLACES} p2 ON d.destination_id = p2.geo_id
            WHERE {where_clause}
              AND COALESCE(p1.province, '') != COALESCE(p2.province, '')
              AND COALESCE(p1.province, '') != ''
              AND COALESCE(p2.province, '') != ''
            GROUP BY send_city, arrive_city
            ORDER BY flow DESC
            LIMIT ?
        """

        inter_rows = conn.execute(inter_query, params + [topk_inter]).fetchall()

        # æ„å»ºçœå†…èµ°å»Šç»“æœ
        if intra_rows:
            intra_df = pd.DataFrame(
                [
                    {
                        "send_city": str(r["send_city"]),
                        "arrive_city": str(r["arrive_city"]),
                        "flow": float(r["flow"]) if r["flow"] is not None else 0.0,
                    }
                    for r in intra_rows
                ]
            )
            intra_df["rank"] = range(1, len(intra_df) + 1)
        else:
            intra_df = pd.DataFrame(
                columns=["send_city", "arrive_city", "flow", "rank"]
            )

        # æ„å»ºçœé™…èµ°å»Šç»“æœ
        if inter_rows:
            inter_df = pd.DataFrame(
                [
                    {
                        "send_city": str(r["send_city"]),
                        "arrive_city": str(r["arrive_city"]),
                        "flow": float(r["flow"]) if r["flow"] is not None else 0.0,
                    }
                    for r in inter_rows
                ]
            )
            inter_df["rank"] = range(1, len(inter_df) + 1)
        else:
            inter_df = pd.DataFrame(
                columns=["send_city", "arrive_city", "flow", "rank"]
            )

        return {"intra_province": intra_df, "inter_province": inter_df}


def benchmark_province_flow_performance(
    start: str,
    end: str,
    direction: str = "send",
    dyna_type: Optional[str] = None,
    iterations: int = 3,
) -> Dict[str, float]:
    """
    æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼šæ¯”è¾ƒä¼˜åŒ–ç‰ˆæœ¬å’ŒåŸå§‹ç‰ˆæœ¬çš„æ‰§è¡Œæ—¶é—´

    Returns:
        Dict with performance metrics
    """
    import time

    print("ğŸš€ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•...")

    # æµ‹è¯•ä¼˜åŒ–ç‰ˆæœ¬
    print("ğŸ“Š æµ‹è¯•ä¼˜åŒ–ç‰ˆæœ¬...")
    optimized_times = []
    for i in range(iterations):
        start_time = time.time()
        result_opt = analyze_province_flow_optimized(
            "test", start, end, "total", direction, dyna_type
        )
        end_time = time.time()
        optimized_times.append(end_time - start_time)
        print(f"  ç¬¬{i+1}æ¬¡: {end_time - start_time:.3f}ç§’")

    # æµ‹è¯•åŸå§‹ç‰ˆæœ¬
    print("ğŸ“Š æµ‹è¯•åŸå§‹ç‰ˆæœ¬...")
    original_times = []
    for i in range(iterations):
        start_time = time.time()
        result_orig = analyze_province_flow_original(
            "test", start, end, "total", direction, dyna_type
        )
        end_time = time.time()
        original_times.append(end_time - start_time)
        print(f"  ç¬¬{i+1}æ¬¡: {end_time - start_time:.3f}ç§’")

    # è®¡ç®—å¹³å‡æ—¶é—´
    avg_optimized = sum(optimized_times) / len(optimized_times)
    avg_original = sum(original_times) / len(original_times)
    speedup = avg_original / avg_optimized if avg_optimized > 0 else 0

    print(f"\nğŸ“ˆ æ€§èƒ½æµ‹è¯•ç»“æœ:")
    print(f"  ä¼˜åŒ–ç‰ˆæœ¬å¹³å‡æ—¶é—´: {avg_optimized:.3f}ç§’")
    print(f"  åŸå§‹ç‰ˆæœ¬å¹³å‡æ—¶é—´: {avg_original:.3f}ç§’")
    print(f"  æ€§èƒ½æå‡: {speedup:.2f}x")
    print(f"  æ—¶é—´èŠ‚çœ: {((avg_original - avg_optimized) / avg_original * 100):.1f}%")

    # éªŒè¯ç»“æœä¸€è‡´æ€§
    if not result_opt.empty and not result_orig.empty:
        # æ¯”è¾ƒç»“æœ
        opt_sorted = result_opt.sort_values(["province"]).reset_index(drop=True)
        orig_sorted = result_orig.sort_values(["province"]).reset_index(drop=True)

        # æ£€æŸ¥çœä»½åˆ—è¡¨æ˜¯å¦ä¸€è‡´
        provinces_match = set(opt_sorted["province"]) == set(orig_sorted["province"])
        print(f"  ç»“æœä¸€è‡´æ€§: {'âœ… é€šè¿‡' if provinces_match else 'âŒ å¤±è´¥'}")

        if provinces_match:
            # æ£€æŸ¥æµé‡æ€»å’Œæ˜¯å¦æ¥è¿‘
            flow_diff = abs(opt_sorted["flow"].sum() - orig_sorted["flow"].sum())
            flow_match = flow_diff < 0.01
            print(
                f"  æµé‡ä¸€è‡´æ€§: {'âœ… é€šè¿‡' if flow_match else 'âŒ å¤±è´¥'} (å·®å¼‚: {flow_diff:.6f})"
            )

    return {
        "optimized_avg": avg_optimized,
        "original_avg": avg_original,
        "speedup": speedup,
        "time_saved_percent": (avg_original - avg_optimized) / avg_original * 100,
    }
